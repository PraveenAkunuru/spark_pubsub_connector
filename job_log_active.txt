:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.arrow#arrow-c-data added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-a633a388-60bf-40d8-9ab1-228920c45e00;1.0
	confs: [default]
	found org.apache.arrow#arrow-c-data;15.0.2 in central
	found org.apache.arrow#arrow-vector;15.0.2 in central
	found org.apache.arrow#arrow-format;15.0.2 in central
	found com.google.flatbuffers#flatbuffers-java;23.5.26 in central
	found org.apache.arrow#arrow-memory-core;15.0.2 in central
	found com.google.code.findbugs#jsr305;3.0.2 in central
	found org.slf4j#slf4j-api;2.0.9 in central
	found com.fasterxml.jackson.core#jackson-core;2.16.0 in central
	found com.fasterxml.jackson.core#jackson-annotations;2.16.0 in central
	found com.fasterxml.jackson.core#jackson-databind;2.16.0 in central
	found com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.16.0 in central
	found commons-codec#commons-codec;1.15 in central
	found org.eclipse.collections#eclipse-collections;11.1.0 in central
	found org.eclipse.collections#eclipse-collections-api;11.1.0 in central
:: resolution report :: resolve 442ms :: artifacts dl 13ms
	:: modules in use:
	com.fasterxml.jackson.core#jackson-annotations;2.16.0 from central in [default]
	com.fasterxml.jackson.core#jackson-core;2.16.0 from central in [default]
	com.fasterxml.jackson.core#jackson-databind;2.16.0 from central in [default]
	com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.16.0 from central in [default]
	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
	com.google.flatbuffers#flatbuffers-java;23.5.26 from central in [default]
	commons-codec#commons-codec;1.15 from central in [default]
	org.apache.arrow#arrow-c-data;15.0.2 from central in [default]
	org.apache.arrow#arrow-format;15.0.2 from central in [default]
	org.apache.arrow#arrow-memory-core;15.0.2 from central in [default]
	org.apache.arrow#arrow-vector;15.0.2 from central in [default]
	org.eclipse.collections#eclipse-collections;11.1.0 from central in [default]
	org.eclipse.collections#eclipse-collections-api;11.1.0 from central in [default]
	org.slf4j#slf4j-api;2.0.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-a633a388-60bf-40d8-9ab1-228920c45e00
	confs: [default]
	0 artifacts copied, 14 already retrieved (0kB/8ms)
25/12/25 20:32:06 INFO SparkEnv: Registering MapOutputTracker
25/12/25 20:32:06 INFO SparkEnv: Registering BlockManagerMaster
25/12/25 20:32:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/12/25 20:32:06 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/25 20:32:06 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties
25/12/25 20:32:06 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
25/12/25 20:32:06 INFO MetricsSystemImpl: google-hadoop-file-system metrics system started
25/12/25 20:32:06 INFO DataprocSparkPlugin: Registered 189 driver metrics
25/12/25 20:32:07 INFO DataprocSparkPlugin: Registered 189 executor metrics
25/12/25 20:32:07 INFO GhfsGlobalStorageStatistics: periodic connector metrics: {gcs_api_client_non_found_response_count=1, gcs_api_client_side_error_count=1, gcs_api_time=236, gcs_api_total_request_count=2, gcs_connector_time=338, gcs_list_file_request=1, gcs_list_file_request_duration=120, gcs_list_file_request_max=120, gcs_list_file_request_mean=120, gcs_list_file_request_min=120, gcs_metadata_request=1, gcs_metadata_request_duration=116, gcs_metadata_request_max=116, gcs_metadata_request_mean=116, gcs_metadata_request_min=116, gs_filesystem_create=3, gs_filesystem_initialize=2, op_get_file_status=1, op_get_file_status_duration=338, op_get_file_status_max=338, op_get_file_status_mean=338, op_get_file_status_min=338, uptimeSeconds=2} [CONTEXT ratelimit_period="5 MINUTES" ]
25/12/25 20:32:07 INFO GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
25/12/25 20:32:08 INFO GoogleHadoopOutputStream: hflush(): No-op due to rate limit (RateLimiter[stableRate=0.2qps]): readers will *not* yet see flushed data for gs://dataproc-temp-us-central1-52979430671-afo0flan/c87bf96e-addc-49b1-ba29-a571d19560bb/spark-job-history/local-1766694726886.inprogress [CONTEXT ratelimit_period="1 MINUTES" ]
Starting Load Generator: Topic=benchmark-throughput-1kb, Count=1000, Size=1024B
25/12/25 20:32:10 INFO PubSubTableProvider: Creating PubSubTable for subscription: unknown
25/12/25 20:32:12 INFO PubSubDataWriter: PubSubDataWriter created for /benchmark-throughput-1kb, partitionId: 0, taskId: 4
25/12/25 20:32:12 INFO PubSubDataWriter: PubSubDataWriter created for /benchmark-throughput-1kb, partitionId: 2, taskId: 6
25/12/25 20:32:12 INFO PubSubDataWriter: PubSubDataWriter created for /benchmark-throughput-1kb, partitionId: 3, taskId: 7
25/12/25 20:32:12 INFO PubSubDataWriter: PubSubDataWriter created for /benchmark-throughput-1kb, partitionId: 1, taskId: 5
Rust: JNI Logging initialized via GlobalRef.
25/12/25 20:32:12 INFO NativePubSub: Rust: NativeWriter.init called for project: 
25/12/25 20:32:12 INFO NativePubSub: Rust: NativeWriter.init called for project: 
25/12/25 20:32:12 INFO NativePubSub: Rust: NativeWriter.init called for project: 
25/12/25 20:32:12 INFO NativePubSub: Rust: NativeWriter.init called for project: 
25/12/25 20:32:12 INFO NativePubSub: Rust: Loading explicit CA roots from /etc/ssl/certs/ca-certificates.crt
Rust Raw: Connecting to gRPC endpoint: https://pubsub.googleapis.com
Rust Raw: Connected to gRPC endpoint: https://pubsub.googleapis.com
Rust Raw: Acquired token (len=1031)
25/12/25 20:32:13 INFO NativePubSub: Rust Auth: Acquired token (length: 1031, prefix: Bearer ya2)
Rust Raw: create_channel_and_header success
Rust Raw: Acquired token (len=1031)
25/12/25 20:32:13 INFO NativePubSub: Rust Auth: Acquired token (length: 1031, prefix: Bearer ya2)
Rust Raw: create_channel_and_header success
25/12/25 20:32:13 INFO BaseAllocator: Debug mode disabled. Enable with the VM option -Darrow.memory.debug.allocator=true.
25/12/25 20:32:13 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
25/12/25 20:32:13 INFO CheckAllocator: Using DefaultAllocationManager at memory-netty-14.0.2.jar!/org/apache/arrow/memory/DefaultAllocationManagerFactory.class
Rust Raw: Acquired token (len=1031)
25/12/25 20:32:13 INFO NativePubSub: Rust Auth: Acquired token (length: 1031, prefix: Bearer ya2)
Rust Raw: create_channel_and_header success
25/12/25 20:32:13 ERROR NativePubSub: Rust: Fatal Publish Error: Status { code: InvalidArgument, message: "You have passed an invalid argument to the service (argument=).", metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Thu, 25 Dec 2025 20:32:13 GMT"} }, source: None }. Stopping background task.
25/12/25 20:32:13 ERROR NativePubSub: Rust: Flush wait failed (channel closed): RecvError(())
25/12/25 20:32:13 ERROR NativePubSub: Rust: Writer close flush failed: Flush channel closed (background task died?)
25/12/25 20:32:13 ERROR PubSubDataWriter: NativeWriter.close failed with code -1
25/12/25 20:32:13 INFO PubSubDataWriter: PubSubDataWriter closed for partitionId: 2
25/12/25 20:32:13 ERROR NativePubSub: Rust: Fatal Publish Error: Status { code: InvalidArgument, message: "You have passed an invalid argument to the service (argument=).", metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Thu, 25 Dec 2025 20:32:13 GMT"} }, source: None }. Stopping background task.
25/12/25 20:32:13 ERROR PubSubDataWriter: NativeWriter.close failed with code -1
25/12/25 20:32:13 ERROR NativePubSub: Rust: Flush wait failed (channel closed): RecvError(())
25/12/25 20:32:13 INFO PubSubDataWriter: PubSubDataWriter closed for partitionId: 3
25/12/25 20:32:13 ERROR NativePubSub: Rust: Writer close flush failed: Flush channel closed (background task died?)
25/12/25 20:32:13 ERROR NativePubSub: Rust: Fatal Publish Error: Status { code: InvalidArgument, message: "You have passed an invalid argument to the service (argument=).", metadata: MetadataMap { headers: {"content-type": "application/grpc", "date": "Thu, 25 Dec 2025 20:32:13 GMT"} }, source: None }. Stopping background task.
25/12/25 20:32:13 ERROR NativePubSub: Rust: Flush wait failed (channel closed): RecvError(())
25/12/25 20:32:13 ERROR PubSubDataWriter: NativeWriter.close failed with code -1
25/12/25 20:32:13 ERROR NativePubSub: Rust: Writer close flush failed: Flush channel closed (background task died?)
25/12/25 20:32:13 INFO PubSubDataWriter: PubSubDataWriter closed for partitionId: 1
25/12/25 20:32:13 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 6)
java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189) ~[spark-pubsub-connector-latest.jar:0.1.0]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.3.jar:3.5.3]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.base/java.lang.Thread.run(Thread.java:829) [?:?]
25/12/25 20:32:13 ERROR Executor: Exception in task 3.0 in stage 2.0 (TID 7)
java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189) ~[spark-pubsub-connector-latest.jar:0.1.0]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.3.jar:3.5.3]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.base/java.lang.Thread.run(Thread.java:829) [?:?]
25/12/25 20:32:13 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 5)
java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189) ~[spark-pubsub-connector-latest.jar:0.1.0]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.3.jar:3.5.3]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.base/java.lang.Thread.run(Thread.java:829) [?:?]
25/12/25 20:32:13 WARN TaskSetManager: Lost task 2.0 in stage 2.0 (TID 6) (cluster-aaf3-m.us-central1-c.c.pakunuru-1119-20250930202256.internal executor driver): java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

25/12/25 20:32:13 ERROR TaskSetManager: Task 2 in stage 2.0 failed 1 times; aborting job
25/12/25 20:32:13 INFO PubSubDataWriter: PubSubDataWriter created for /benchmark-throughput-1kb, partitionId: 4, taskId: 8
25/12/25 20:32:13 INFO NativePubSub: Rust: NativeWriter.init called for project: 
25/12/25 20:32:13 ERROR AppendDataExec: Data source write support com.google.cloud.spark.pubsub.PubSubBatchWrite@426fb1c3 is aborting.
25/12/25 20:32:13 ERROR AppendDataExec: Data source write support com.google.cloud.spark.pubsub.PubSubBatchWrite@426fb1c3 aborted.
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 6) (cluster-aaf3-m.us-central1-c.c.pakunuru-1119-20250930202256.internal executor driver): java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2456)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:405)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:379)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:232)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:344)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:343)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:232)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:108)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:99)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:473)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:473)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:449)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:99)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:143)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:315)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at com.google.cloud.spark.pubsub.benchmark.PubSubLoadGenerator$.main(PubSubLoadGenerator.scala:47)
	at com.google.cloud.spark.pubsub.benchmark.PubSubLoadGenerator.main(PubSubLoadGenerator.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1216)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:218)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:92)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1314)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1323)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Rust Raw: Acquired token (len=1031)
25/12/25 20:32:13 INFO NativePubSub: Rust Auth: Acquired token (length: 1031, prefix: Bearer ya2)
Rust Raw: create_channel_and_header success
25/12/25 20:32:13 ERROR Utils: Aborting task
org.apache.spark.TaskKilledException: null
	at org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) [spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) [spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) [spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.3.jar:3.5.3]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.base/java.lang.Thread.run(Thread.java:829) [?:?]
25/12/25 20:32:13 ERROR DataWritingSparkTask: Aborting commit for partition 4 (task 8, attempt 0, stage 2.0)
25/12/25 20:32:13 INFO NativePubSub: Rust: Flush completed.
25/12/25 20:32:13 INFO NativePubSub: Rust: Publisher background task ended
25/12/25 20:32:13 INFO PubSubDataWriter: PubSubDataWriter closed for partitionId: 4
25/12/25 20:32:13 ERROR DataWritingSparkTask: Aborted commit for partition 4 (task 8, attempt 0, stage 2.0)
25/12/25 20:32:13 WARN TaskSetManager: Lost task 4.0 in stage 2.0 (TID 8) (cluster-aaf3-m.us-central1-c.c.pakunuru-1119-20250930202256.internal executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 6) (cluster-aaf3-m.us-central1-c.c.pakunuru-1119-20250930202256.internal executor driver): java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:)
Rust Raw: Acquired token (len=1031)
25/12/25 20:32:13 INFO NativePubSub: Rust Auth: Acquired token (length: 1031, prefix: Bearer ya2)
Rust Raw: create_channel_and_header success
25/12/25 20:32:13 ERROR Utils: Aborting task
org.apache.spark.TaskKilledException: null
	at org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1399) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) [spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) [spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96) [spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.3.jar:3.5.3]
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
	at java.base/java.lang.Thread.run(Thread.java:829) [?:?]
25/12/25 20:32:13 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 4, attempt 0, stage 2.0)
25/12/25 20:32:13 INFO PubSubDataWriter: PubSubDataWriter closed for partitionId: 0
25/12/25 20:32:13 INFO NativePubSub: Rust: Flush completed.
25/12/25 20:32:13 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 4, attempt 0, stage 2.0)
25/12/25 20:32:13 INFO NativePubSub: Rust: Publisher background task ended
25/12/25 20:32:13 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4) (cluster-aaf3-m.us-central1-c.c.pakunuru-1119-20250930202256.internal executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 2.0 failed 1 times, most recent failure: Lost task 2.0 in stage 2.0 (TID 6) (cluster-aaf3-m.us-central1-c.c.pakunuru-1119-20250930202256.internal executor driver): java.lang.RuntimeException: NativeWriter.close failed with code -1
	at com.google.cloud.spark.pubsub.PubSubDataWriter.close(PubSubWrite.scala:189)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$13(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1421)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:523)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:459)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:528)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:408)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)

Driver stacktrace:)
